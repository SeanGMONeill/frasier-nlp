{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95032464-8767-41f9-9b75-9e5fe79d6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearn_text\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047a1365-565e-49aa-a56e-ebfcf5c59ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e4cca-754e-43fb-a3da-e0ed931c565f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb62ebb-8caa-4f0e-9778-b9f9282e971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 263 episodes.\n"
     ]
    }
   ],
   "source": [
    "scripts = []\n",
    "for season in range(1,12):\n",
    "    for x in os.listdir('data/site=kacl780/season={}'.format(season)):\n",
    "        path = os.path.join('data/site=kacl780/season={}'.format(season), x)\n",
    "        with open(path, 'r') as f:\n",
    "            script = json.load(f)\n",
    "            script['all_speech'] = ' '.join(list(map(lambda x: x['line'], script['dialogue'])))\n",
    "            scripts.append(script)\n",
    "titles = list(map(lambda x: x['title'], scripts))\n",
    "print('Loaded {} episodes.'.format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15db9691-9ee8-4444-b677-c76b1816ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dialogue(scripts):\n",
    "    return sum(list(map(lambda x: x['dialogue'], scripts)),[])\n",
    "\n",
    "def get_episode(scripts, season, episode):\n",
    "    return list(filter(lambda x: (x['season'] == season) & (x['episode'] == episode), scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00caa264-8473-449b-a738-0124fcfc167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_speech(character, scripts, join=False):\n",
    "    speech = filter(lambda x: x['character'] == character, all_dialogue(scripts))\n",
    "    speech = list(map(lambda x: x['line'].replace(\"\\'\",\"'\"), speech))\n",
    "    if join:\n",
    "        speech = ' '.join(speech)\n",
    "        speech = re.sub(re.compile(' +'), ' ', speech).strip()\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6104e2-db8c-4e27-9f92-0b636826c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_counts = Counter(list(map(lambda y: y['character'], sum(list(map(lambda x: x['dialogue'], scripts)),[]))))\n",
    "top_chars = sorted(line_counts.keys(), key = lambda x:line_counts[x], reverse = True)[:10]\n",
    "top_char_speech = dict(zip(top_chars, list(map(lambda x: char_speech(x, scripts, True), top_chars))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b14d5-4b91-4aa3-a28e-bdfce25522c9",
   "metadata": {},
   "source": [
    "### Create Vectorizer, Fit and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adac27d6-d9a8-4c90-b856-9f202a02c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab2b13ae-db06-4b15-8ada-cf79ca39be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = LemmaTokenizer()(' '.join(stopwords.words('english'))) + ['.',',','?','!','...', 'oh']\n",
    "#stop_words='english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79336d29-4f69-4784-a1b3-c32a1b2b2fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22270"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words,  tokenizer=LemmaTokenizer())\n",
    "#tfidf  = tfidf_vectorizer.fit_transform(list(map(lambda x: x['all_speech'], scripts)))\n",
    "tfidf  = tfidf_vectorizer.fit_transform(top_char_speech.values())\n",
    "len(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1922c088-2e41-4b88-af72-3c7ef94ea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(tfidf.todense(), index=titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#df = pd.DataFrame.sparse.from_spmatrix(tfidf, index=top_char_speech.keys(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df = pd.DataFrame(tfidf.todense(), index=top_char_speech.keys(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "df.to_csv('tfidf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3f345-5b1c-42ce-805f-8d5766a672ca",
   "metadata": {},
   "source": [
    "### Process Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc3a3f8d-d6e5-4e2e-8d43-318051929621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['frasier', ''m', 'well', 'know', 'get', 'nile', 'right', 'like', 'hey',\n",
       "       'go', 'got', 'one', '``', '''', 'come', 'daphne', 'roz', 'look', 'see',\n",
       "       'going', 'yes', 'yeah', 'think', 'want', 'back', 'time', 'little',\n",
       "       'thing', 'good', 'crane'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgs = df.mean(axis=0)\n",
    "avgs.name = '#Average'\n",
    "df.append(avgs).sort_values(by='#Average', ascending=False, axis=1).columns[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea42f8a9-b387-4532-8e40-02ed8ba9d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_by_char(character, n, df):\n",
    "    return df.append(avgs).sort_values(by=character, ascending=False, axis=1).columns[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "169c7cb5-1571-4bb2-af66-61c9d97c89a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frasier</th>\n",
       "      <th>'m</th>\n",
       "      <th>well</th>\n",
       "      <th>know</th>\n",
       "      <th>get</th>\n",
       "      <th>nile</th>\n",
       "      <th>right</th>\n",
       "      <th>like</th>\n",
       "      <th>hey</th>\n",
       "      <th>go</th>\n",
       "      <th>...</th>\n",
       "      <th>corruption</th>\n",
       "      <th>podiatrist</th>\n",
       "      <th>sandbag</th>\n",
       "      <th>sandalwood</th>\n",
       "      <th>fourteen-dollar-a-pound</th>\n",
       "      <th>sugary</th>\n",
       "      <th>mover</th>\n",
       "      <th>sugarcoating</th>\n",
       "      <th>movie-ending</th>\n",
       "      <th>batter-dipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Frasier</th>\n",
       "      <td>0.133717</td>\n",
       "      <td>0.273039</td>\n",
       "      <td>0.391624</td>\n",
       "      <td>0.259813</td>\n",
       "      <td>0.117801</td>\n",
       "      <td>0.299379</td>\n",
       "      <td>0.183707</td>\n",
       "      <td>0.110852</td>\n",
       "      <td>0.009555</td>\n",
       "      <td>0.122845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Niles</th>\n",
       "      <td>0.358199</td>\n",
       "      <td>0.290155</td>\n",
       "      <td>0.344922</td>\n",
       "      <td>0.182557</td>\n",
       "      <td>0.131109</td>\n",
       "      <td>0.135811</td>\n",
       "      <td>0.135535</td>\n",
       "      <td>0.108704</td>\n",
       "      <td>0.042178</td>\n",
       "      <td>0.123088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Martin</th>\n",
       "      <td>0.226298</td>\n",
       "      <td>0.227914</td>\n",
       "      <td>0.339770</td>\n",
       "      <td>0.263475</td>\n",
       "      <td>0.190737</td>\n",
       "      <td>0.197849</td>\n",
       "      <td>0.163904</td>\n",
       "      <td>0.160025</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>0.153559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daphne</th>\n",
       "      <td>0.096808</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.279035</td>\n",
       "      <td>0.198276</td>\n",
       "      <td>0.158931</td>\n",
       "      <td>0.208629</td>\n",
       "      <td>0.148577</td>\n",
       "      <td>0.167214</td>\n",
       "      <td>0.026728</td>\n",
       "      <td>0.149095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roz</th>\n",
       "      <td>0.417029</td>\n",
       "      <td>0.271277</td>\n",
       "      <td>0.251050</td>\n",
       "      <td>0.242722</td>\n",
       "      <td>0.170738</td>\n",
       "      <td>0.113627</td>\n",
       "      <td>0.121956</td>\n",
       "      <td>0.145157</td>\n",
       "      <td>0.132143</td>\n",
       "      <td>0.154081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bulldog</th>\n",
       "      <td>0.151273</td>\n",
       "      <td>0.205299</td>\n",
       "      <td>0.091844</td>\n",
       "      <td>0.143169</td>\n",
       "      <td>0.116156</td>\n",
       "      <td>0.029714</td>\n",
       "      <td>0.121559</td>\n",
       "      <td>0.132364</td>\n",
       "      <td>0.470289</td>\n",
       "      <td>0.129663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lilith</th>\n",
       "      <td>0.462776</td>\n",
       "      <td>0.251508</td>\n",
       "      <td>0.251508</td>\n",
       "      <td>0.125754</td>\n",
       "      <td>0.085513</td>\n",
       "      <td>0.130784</td>\n",
       "      <td>0.105634</td>\n",
       "      <td>0.095573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kenny</th>\n",
       "      <td>0.182155</td>\n",
       "      <td>0.277786</td>\n",
       "      <td>0.200371</td>\n",
       "      <td>0.154832</td>\n",
       "      <td>0.145724</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.104739</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>0.338998</td>\n",
       "      <td>0.091078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bebe</th>\n",
       "      <td>0.428323</td>\n",
       "      <td>0.227716</td>\n",
       "      <td>0.162654</td>\n",
       "      <td>0.140967</td>\n",
       "      <td>0.097593</td>\n",
       "      <td>0.048796</td>\n",
       "      <td>0.059640</td>\n",
       "      <td>0.151811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Donny</th>\n",
       "      <td>0.178606</td>\n",
       "      <td>0.290234</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.275350</td>\n",
       "      <td>0.141396</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>0.163722</td>\n",
       "      <td>0.104187</td>\n",
       "      <td>0.080417</td>\n",
       "      <td>0.096745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Average</th>\n",
       "      <td>0.263518</td>\n",
       "      <td>0.259293</td>\n",
       "      <td>0.249883</td>\n",
       "      <td>0.198692</td>\n",
       "      <td>0.135570</td>\n",
       "      <td>0.135230</td>\n",
       "      <td>0.130897</td>\n",
       "      <td>0.130795</td>\n",
       "      <td>0.129245</td>\n",
       "      <td>0.121136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 22270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           frasier        'm      well      know       get      nile  \\\n",
       "Frasier   0.133717  0.273039  0.391624  0.259813  0.117801  0.299379   \n",
       "Niles     0.358199  0.290155  0.344922  0.182557  0.131109  0.135811   \n",
       "Martin    0.226298  0.227914  0.339770  0.263475  0.190737  0.197849   \n",
       "Daphne    0.096808  0.278000  0.279035  0.198276  0.158931  0.208629   \n",
       "Roz       0.417029  0.271277  0.251050  0.242722  0.170738  0.113627   \n",
       "Bulldog   0.151273  0.205299  0.091844  0.143169  0.116156  0.029714   \n",
       "Lilith    0.462776  0.251508  0.251508  0.125754  0.085513  0.130784   \n",
       "Kenny     0.182155  0.277786  0.200371  0.154832  0.145724  0.009108   \n",
       "Bebe      0.428323  0.227716  0.162654  0.140967  0.097593  0.048796   \n",
       "Donny     0.178606  0.290234  0.186047  0.275350  0.141396  0.178606   \n",
       "#Average  0.263518  0.259293  0.249883  0.198692  0.135570  0.135230   \n",
       "\n",
       "             right      like       hey        go  ...  corruption  podiatrist  \\\n",
       "Frasier   0.183707  0.110852  0.009555  0.122845  ...    0.000303    0.000303   \n",
       "Niles     0.135535  0.108704  0.042178  0.123088  ...    0.000000    0.000000   \n",
       "Martin    0.163904  0.160025  0.192137  0.153559  ...    0.000000    0.000000   \n",
       "Daphne    0.148577  0.167214  0.026728  0.149095  ...    0.000000    0.000000   \n",
       "Roz       0.121956  0.145157  0.132143  0.154081  ...    0.000000    0.000000   \n",
       "Bulldog   0.121559  0.132364  0.470289  0.129663  ...    0.000000    0.000000   \n",
       "Lilith    0.105634  0.095573  0.000000  0.120724  ...    0.000000    0.000000   \n",
       "Kenny     0.104739  0.132062  0.338998  0.091078  ...    0.000000    0.000000   \n",
       "Bebe      0.059640  0.151811  0.000000  0.070484  ...    0.000000    0.000000   \n",
       "Donny     0.163722  0.104187  0.080417  0.096745  ...    0.000000    0.000000   \n",
       "#Average  0.130897  0.130795  0.129245  0.121136  ...    0.000030    0.000030   \n",
       "\n",
       "           sandbag  sandalwood  fourteen-dollar-a-pound    sugary     mover  \\\n",
       "Frasier   0.000303    0.000303                 0.000303  0.000303  0.000303   \n",
       "Niles     0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Martin    0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Daphne    0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Roz       0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Bulldog   0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Lilith    0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Kenny     0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Bebe      0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "Donny     0.000000    0.000000                 0.000000  0.000000  0.000000   \n",
       "#Average  0.000030    0.000030                 0.000030  0.000030  0.000030   \n",
       "\n",
       "          sugarcoating  movie-ending  batter-dipped  \n",
       "Frasier       0.000303      0.000303       0.000303  \n",
       "Niles         0.000000      0.000000       0.000000  \n",
       "Martin        0.000000      0.000000       0.000000  \n",
       "Daphne        0.000000      0.000000       0.000000  \n",
       "Roz           0.000000      0.000000       0.000000  \n",
       "Bulldog       0.000000      0.000000       0.000000  \n",
       "Lilith        0.000000      0.000000       0.000000  \n",
       "Kenny         0.000000      0.000000       0.000000  \n",
       "Bebe          0.000000      0.000000       0.000000  \n",
       "Donny         0.000000      0.000000       0.000000  \n",
       "#Average      0.000030      0.000030       0.000030  \n",
       "\n",
       "[11 rows x 22270 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.append(avgs).sort_values(by='#Average', ascending=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e67f732c-7a42-4267-8708-7b7bce582026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frasier: well, nile, 'm, know, yes, dad, right, roz, frasier, go, see, get, '', ``, daphne, like, think, one, come, good\n",
      "Niles: frasier, well, 'm, daphne, dad, know, yes, ``, '', mari, nile, right, get, one, go, going, like, think, look, see\n",
      "Martin: well, know, 'm, frasier, yeah, nile, hey, get, right, like, got, go, come, eddie, look, guy, one, ', daphne, going\n",
      "Daphne: crane, well, 'm, dr., nile, know, like, get, go, right, look, one, yes, daphne, think, ``, '', going, come, frasier\n",
      "Roz: frasier, 'm, well, know, get, go, like, ``, '', one, hey, got, really, right, look, yeah, nile, going, guy, think\n",
      "Bulldog: hey, doc, got, roz, 'm, bulldog, frasier, yeah, know, like, go, guy, right, get, ``, '', back, look, come, one\n",
      "Lilith: frasier, well, 'm, lilith, yes, frederick, nile, know, go, u, thank, see, right, think, ``, '', brian, like, one, time\n",
      "Kenny: hey, doc, 'm, well, frasier, got, know, yeah, get, roz, kenny, like, great, show, one, ``, '', right, look, guy\n",
      "Bebe: frasier, 'm, bebe, darling, well, like, little, want, know, one, make, see, [, let, come, ], back, get, client, roz\n",
      "Donny: 'm, know, daphne, got, well, frasier, nile, yeah, right, come, get, honey, going, see, guy, little, like, maria, hi, donny\n"
     ]
    }
   ],
   "source": [
    "for char in top_chars:\n",
    "    print(char + ': '+ ', '.join(list(top_n_by_char(char, 20, df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdee94-c368-4a56-82df-d4bd18941543",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a19a0c7-db32-4cf2-9ecd-808d5a84476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_episode(scripts, 7, 19)\n",
    "#char_speech('Bebe', scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678be7-3198-4ba9-a60b-360aa2e04e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
